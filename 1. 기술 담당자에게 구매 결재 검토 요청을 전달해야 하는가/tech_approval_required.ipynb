{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'./orders_with_predicted_value.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of rows in dataset: {df.shape[0]}')\n",
    "print(df[df.columns[0]].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Get the data into the right shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data = pd.get_dummies(df)\n",
    "encoded_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs = encoded_data.corr()['tech_approval_required'].abs()\n",
    "columns = corrs[corrs > .1].index\n",
    "corrs = corrs.filter(columns)\n",
    "corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encoded_data = encoded_data[columns]\n",
    "encoded_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Create training, validation and test data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_and_test_data = train_test_split(encoded_data, test_size=0.3, random_state=0)\n",
    "val_df, test_df = train_test_split(val_and_test_data, test_size=0.333, random_state=0)\n",
    "\n",
    "train_data = train_df.to_csv(None, header=False, index=False).encode()\n",
    "val_data = val_df.to_csv(None, header=False, index=False).encode()\n",
    "test_data = test_df.to_csv(None, header=True, index=False).encode()\n",
    "\n",
    "with s3.open(f'{data_bucket}/{subfolder}/processed/train.csv', 'wb') as f:\n",
    "    f.write(train_data)\n",
    "\n",
    "with s3.open(f'{data_bucket}/{subfolder}/processed/val.csv', 'wb') as f:\n",
    "    f.write(val_data)\n",
    "    \n",
    "with s3.open(f'{data_bucket}/{subfolder}/processed/test.csv', 'wb') as f:\n",
    "    f.write(test_data) \n",
    "    \n",
    "train_input = sagemaker.TrainingInput(s3_data=f's3://{data_bucket}/{subfolder}/processed/train.csv', content_type='csv')\n",
    "val_input = sagemaker.TrainingInput(s3_data=f's3://{data_bucket}/{subfolder}/processed/val.csv', content_type='csv')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "\n",
    "container = sagemaker.image_uris.retrieve(\n",
    "                'xgboost',\n",
    "                boto3.Session().region_name,\n",
    "                'latest')\n",
    "\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "                container,\n",
    "                role,\n",
    "                instance_count=1, \n",
    "                instance_type='ml.m4.xlarge',\n",
    "                output_path=f's3://{data_bucket}/{subfolder}/output',\n",
    "                sagemaker_session=sess)\n",
    "\n",
    "estimator.set_hyperparameters(\n",
    "                max_depth=5,\n",
    "                subsample=0.7,\n",
    "                objective='binary:logistic',\n",
    "                eval_metric = 'auc',\n",
    "                num_round=100,\n",
    "                early_stopping_rounds=10)\n",
    "\n",
    "estimator.fit({'train': train_input, 'validation': val_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Host the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = 'order-approval'\n",
    "try:\n",
    "    sess.delete_endpoint(endpoint_name)\n",
    "    print('Warning: Existing endpoint deleted to make way for your new endpoint.')\n",
    "    sleep(30)\n",
    "except:\n",
    "    pass    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = estimator.deploy(initial_instance_count=1,\n",
    "               instance_type='ml.m4.xlarge', \n",
    "               endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import CSVSerializer\n",
    "predictor.serializer = CSVSerializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(row):\n",
    "    prediction = round(float(predictor.predict(row[1:]).decode('utf-8')))\n",
    "    return prediction\n",
    "\n",
    "with s3.open(f'{data_bucket}/{subfolder}/processed/test.csv') as f:\n",
    "    test_data = pd.read_csv(f)\n",
    "\n",
    "cols = list(test_data.columns)\n",
    "test_data['prediction'] = test_data.apply(get_prediction, axis=1)\n",
    "test_data = test_data[['prediction'] + cols]\n",
    "test_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(test_data['prediction'] == test_data['tech_approval_required']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove the Endpoint (optional)\n",
    "Comment out this cell to remove the endpoint if you want the endpoint to exist after \"run all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.delete_endpoint(endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
